{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cpu')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.genetic_generator import decode_bracket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "\n",
    "TODO: добавить glob, переместить все дампы в папку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810 files\n"
     ]
    }
   ],
   "source": [
    "files_x = sorted(glob.glob('dumps/dump_x*.pkl'))\n",
    "files_y = sorted(glob.glob('dumps/dump_y*.pkl'))\n",
    "print(f'{len(files_x)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files_x[0], 'rb') as fx, open(files_y[0], 'rb') as fy:\n",
    "    data_x = np.array([decode_bracket(i)[0] for i in pickle.load(fx)])\n",
    "    data_y = np.array([decode_bracket(i)[0] for i in pickle.load(fy)])\n",
    "    assert data_x.shape == data_y.shape\n",
    "    assert len(data_x.shape) == 3\n",
    "\n",
    "files_x.pop(0)\n",
    "files_y.pop(0)\n",
    "\n",
    "for filename_x, filename_y in zip(files_x, files_y):\n",
    "    with open(filename_x, 'rb') as fx, open(filename_y, 'rb') as fy:\n",
    "        ex_data_x = np.array([decode_bracket(i)[0] for i in pickle.load(fx)])\n",
    "        ex_data_y = np.array([decode_bracket(i)[0] for i in pickle.load(fy)])\n",
    "\n",
    "        assert ex_data_x.shape == ex_data_y.shape, f'Broke: {ex_data_x}, {ex_data_y}, ex_data_x.shape == ex_data_y.shape: {ex_data_x.shape == ex_data_y.shape}'\n",
    "        assert len(ex_data_x.shape) == 3\n",
    "\n",
    "        data_x = np.vstack((data_x, ex_data_x))\n",
    "        data_y = np.vstack((data_y, ex_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  8]\n",
      " [56 69]\n",
      " [59 69]\n",
      " ...\n",
      " [34 38]\n",
      " [44 55]\n",
      " [45 55]]\n"
     ]
    }
   ],
   "source": [
    "data_x_r = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2]))\n",
    "data_y_r = data_y.reshape((data_y.shape[0], data_y.shape[1] * data_y.shape[2]))\n",
    "swaps = []\n",
    "\n",
    "for x, y in zip(data_x_r, data_y_r):\n",
    "    swaps.append(np.where(x != y)[0])\n",
    "swaps = np.array(swaps)\n",
    "print(swaps)\n",
    "assert swaps.shape[0] == data_x.shape[0]\n",
    "assert swaps.shape[1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 19510, test: 2168\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(data_x_r.astype('float32'))\n",
    "y = torch.tensor(swaps.astype('float32'))\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(f'Train: {X.shape[0]}, test: {X_test.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полносвязная нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# test activation functions\n",
    "# test architectures\n",
    "# different optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=70, out_features=17, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=17, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Linear(X.shape[1], X.shape[1]),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(X.shape[1], X.shape[1]),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(X.shape[1], X.shape[1]),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(X.shape[1], X.shape[1] // 4),  \n",
    "  nn.ReLU(),\n",
    "  nn.Linear(X.shape[1] // 4, 2),\n",
    "  )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/100, latest loss 639.026, R^2 on test -0.787, NMSE on test 7.09e+02\n",
      "Finished epoch 2/100, latest loss 596.880, R^2 on test -0.655, NMSE on test 6.57e+02\n",
      "Finished epoch 3/100, latest loss 562.496, R^2 on test -0.577, NMSE on test 6.26e+02\n",
      "Finished epoch 4/100, latest loss 534.467, R^2 on test -0.466, NMSE on test 5.82e+02\n",
      "Finished epoch 5/100, latest loss 511.640, R^2 on test -0.400, NMSE on test 5.56e+02\n",
      "Finished epoch 6/100, latest loss 493.070, R^2 on test -0.339, NMSE on test 5.32e+02\n",
      "Finished epoch 7/100, latest loss 477.980, R^2 on test -0.283, NMSE on test 5.09e+02\n",
      "Finished epoch 8/100, latest loss 465.734, R^2 on test -0.233, NMSE on test 4.89e+02\n",
      "Finished epoch 9/100, latest loss 455.811, R^2 on test -0.207, NMSE on test 4.79e+02\n",
      "Finished epoch 10/100, latest loss 447.784, R^2 on test -0.164, NMSE on test 4.62e+02\n",
      "Finished epoch 11/100, latest loss 441.304, R^2 on test -0.147, NMSE on test 4.55e+02\n",
      "Finished epoch 12/100, latest loss 436.084, R^2 on test -0.126, NMSE on test 4.47e+02\n",
      "Finished epoch 13/100, latest loss 431.890, R^2 on test -0.111, NMSE on test 4.41e+02\n",
      "Finished epoch 14/100, latest loss 428.530, R^2 on test -0.093, NMSE on test 4.34e+02\n",
      "Finished epoch 15/100, latest loss 425.847, R^2 on test -0.093, NMSE on test 4.34e+02\n",
      "Finished epoch 16/100, latest loss 423.714, R^2 on test -0.065, NMSE on test 4.23e+02\n",
      "Finished epoch 17/100, latest loss 422.026, R^2 on test -0.065, NMSE on test 4.23e+02\n",
      "Finished epoch 18/100, latest loss 420.697, R^2 on test -0.052, NMSE on test 4.18e+02\n",
      "Finished epoch 19/100, latest loss 419.659, R^2 on test -0.052, NMSE on test 4.18e+02\n",
      "Finished epoch 20/100, latest loss 418.854, R^2 on test -0.042, NMSE on test 4.14e+02\n",
      "Finished epoch 21/100, latest loss 418.236, R^2 on test -0.042, NMSE on test 4.14e+02\n",
      "Finished epoch 22/100, latest loss 417.768, R^2 on test -0.032, NMSE on test 4.1e+02\n",
      "Finished epoch 23/100, latest loss 417.420, R^2 on test -0.032, NMSE on test 4.1e+02\n",
      "Finished epoch 24/100, latest loss 417.166, R^2 on test -0.032, NMSE on test 4.1e+02\n",
      "Finished epoch 25/100, latest loss 416.987, R^2 on test -0.024, NMSE on test 4.07e+02\n",
      "Finished epoch 26/100, latest loss 416.867, R^2 on test -0.024, NMSE on test 4.07e+02\n",
      "Finished epoch 27/100, latest loss 416.793, R^2 on test -0.024, NMSE on test 4.07e+02\n",
      "Finished epoch 28/100, latest loss 416.754, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 29/100, latest loss 416.742, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 30/100, latest loss 416.750, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 31/100, latest loss 416.774, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 32/100, latest loss 416.808, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 33/100, latest loss 416.849, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 34/100, latest loss 416.896, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 35/100, latest loss 416.945, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 36/100, latest loss 416.996, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 37/100, latest loss 417.047, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 38/100, latest loss 417.097, R^2 on test -0.016, NMSE on test 4.03e+02\n",
      "Finished epoch 39/100, latest loss 417.146, R^2 on test -0.011, NMSE on test 4.01e+02\n",
      "Finished epoch 40/100, latest loss 417.192, R^2 on test -0.011, NMSE on test 4.01e+02\n",
      "Finished epoch 41/100, latest loss 417.237, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 42/100, latest loss 417.279, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 43/100, latest loss 417.319, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 44/100, latest loss 417.357, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 45/100, latest loss 417.392, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 46/100, latest loss 417.424, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 47/100, latest loss 417.454, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 48/100, latest loss 417.482, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 49/100, latest loss 417.508, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 50/100, latest loss 417.531, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 51/100, latest loss 417.553, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 52/100, latest loss 417.573, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 53/100, latest loss 417.592, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 54/100, latest loss 417.609, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 55/100, latest loss 417.624, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 56/100, latest loss 417.638, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 57/100, latest loss 417.651, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 58/100, latest loss 417.663, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 59/100, latest loss 417.673, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 60/100, latest loss 417.683, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 61/100, latest loss 417.692, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 62/100, latest loss 417.700, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 63/100, latest loss 417.707, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 64/100, latest loss 417.714, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 65/100, latest loss 417.720, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 66/100, latest loss 417.726, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 67/100, latest loss 417.731, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 68/100, latest loss 417.735, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 69/100, latest loss 417.740, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 70/100, latest loss 417.743, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 71/100, latest loss 417.747, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 72/100, latest loss 417.750, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 73/100, latest loss 417.753, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 74/100, latest loss 417.755, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 75/100, latest loss 417.758, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 76/100, latest loss 417.760, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 77/100, latest loss 417.762, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 78/100, latest loss 417.763, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 79/100, latest loss 417.765, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 80/100, latest loss 417.766, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 81/100, latest loss 417.768, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 82/100, latest loss 417.769, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 83/100, latest loss 417.770, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 84/100, latest loss 417.771, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 85/100, latest loss 417.772, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 86/100, latest loss 417.773, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 87/100, latest loss 417.773, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 88/100, latest loss 417.774, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 89/100, latest loss 417.775, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 90/100, latest loss 417.775, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 91/100, latest loss 417.776, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 92/100, latest loss 417.776, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 93/100, latest loss 417.776, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 94/100, latest loss 417.777, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 95/100, latest loss 417.777, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 96/100, latest loss 417.778, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 97/100, latest loss 417.778, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 98/100, latest loss 417.778, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 99/100, latest loss 417.778, R^2 on test -0.006, NMSE on test 3.99e+02\n",
      "Finished epoch 100/100, latest loss 417.778, R^2 on test -0.006, NMSE on test 3.99e+02\n"
     ]
    }
   ],
   "source": [
    "for n_epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        \n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(loss.item())\n",
    "    \n",
    "    print(f'Finished epoch {n_epoch + 1}/{n_epochs}, latest loss {loss:.3f}, R^2 on test {r2_score(y_test.detach().numpy(), model(X_test).detach().floor().numpy()):.3f}, NMSE on test {mean_squared_error(y_test.detach().numpy(), model(X_test).detach().floor().numpy()):.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_big_14.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=70, out_features=17, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=17, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_big_14.pkl'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.006"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(r2_score(y_test.detach().numpy(), model(X_test).detach().floor().numpy()), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточная нейронка"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные для нее не reshape'им"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 19510, test: 2168\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(data_x.astype('float32'))\n",
    "X = X[:, None, :, :]\n",
    "y = torch.tensor(swaps.astype('float32'))\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(f'Train: {X.shape[0]}, test: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSwapAdviser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetSwapAdviser, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=4)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=2)\n",
    "        #self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(30, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "\n",
    "        #x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 30)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 6, 11, 2]             102\n",
      "         MaxPool2d-2              [-1, 6, 5, 1]               0\n",
      "            Linear-3                   [-1, 16]             496\n",
      "            Linear-4                    [-1, 8]             136\n",
      "            Linear-5                    [-1, 2]              18\n",
      "================================================================\n",
      "Total params: 752\n",
      "Trainable params: 752\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ConvNetSwapAdviser().to(device)\n",
    "\n",
    "summary(model, (1, 14, 5))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "history = []\n",
    "n_epochs = 150\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/150, \n",
      "          latest loss 547.078, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 2/150, \n",
      "          latest loss 547.049, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 3/150, \n",
      "          latest loss 547.012, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 4/150, \n",
      "          latest loss 546.971, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 5/150, \n",
      "          latest loss 546.908, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 6/150, \n",
      "          latest loss 546.886, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 7/150, \n",
      "          latest loss 546.862, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 8/150, \n",
      "          latest loss 546.812, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 9/150, \n",
      "          latest loss 546.736, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 10/150, \n",
      "          latest loss 546.698, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 11/150, \n",
      "          latest loss 546.674, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 12/150, \n",
      "          latest loss 546.642, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 13/150, \n",
      "          latest loss 546.621, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 14/150, \n",
      "          latest loss 546.593, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 15/150, \n",
      "          latest loss 546.573, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 16/150, \n",
      "          latest loss 546.529, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 17/150, \n",
      "          latest loss 546.415, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 18/150, \n",
      "          latest loss 546.484, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 19/150, \n",
      "          latest loss 546.453, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 20/150, \n",
      "          latest loss 546.338, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 21/150, \n",
      "          latest loss 546.376, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 22/150, \n",
      "          latest loss 546.392, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 23/150, \n",
      "          latest loss 546.377, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 24/150, \n",
      "          latest loss 546.335, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 25/150, \n",
      "          latest loss 546.343, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 26/150, \n",
      "          latest loss 546.280, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 27/150, \n",
      "          latest loss 546.299, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 28/150, \n",
      "          latest loss 546.249, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 29/150, \n",
      "          latest loss 546.229, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 30/150, \n",
      "          latest loss 546.183, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 31/150, \n",
      "          latest loss 546.144, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 32/150, \n",
      "          latest loss 546.195, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 33/150, \n",
      "          latest loss 546.151, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 34/150, \n",
      "          latest loss 546.083, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 35/150, \n",
      "          latest loss 546.043, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 36/150, \n",
      "          latest loss 546.016, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 37/150, \n",
      "          latest loss 545.888, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 38/150, \n",
      "          latest loss 545.866, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 39/150, \n",
      "          latest loss 545.866, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 40/150, \n",
      "          latest loss 545.858, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 41/150, \n",
      "          latest loss 545.799, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 42/150, \n",
      "          latest loss 545.821, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 43/150, \n",
      "          latest loss 545.791, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 44/150, \n",
      "          latest loss 545.753, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 45/150, \n",
      "          latest loss 545.706, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 46/150, \n",
      "          latest loss 545.622, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 47/150, \n",
      "          latest loss 545.835, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 48/150, \n",
      "          latest loss 545.765, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 49/150, \n",
      "          latest loss 545.456, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 50/150, \n",
      "          latest loss 545.453, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 51/150, \n",
      "          latest loss 545.417, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 52/150, \n",
      "          latest loss 545.642, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 53/150, \n",
      "          latest loss 545.566, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 54/150, \n",
      "          latest loss 545.318, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 55/150, \n",
      "          latest loss 545.696, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 56/150, \n",
      "          latest loss 545.294, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 57/150, \n",
      "          latest loss 545.245, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 58/150, \n",
      "          latest loss 545.757, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 59/150, \n",
      "          latest loss 545.669, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 60/150, \n",
      "          latest loss 545.586, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 61/150, \n",
      "          latest loss 545.653, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 62/150, \n",
      "          latest loss 545.681, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 63/150, \n",
      "          latest loss 545.503, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 64/150, \n",
      "          latest loss 545.324, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 65/150, \n",
      "          latest loss 545.490, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 66/150, \n",
      "          latest loss 545.008, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 67/150, \n",
      "          latest loss 544.894, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 68/150, \n",
      "          latest loss 544.839, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 69/150, \n",
      "          latest loss 545.298, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 70/150, \n",
      "          latest loss 545.298, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 71/150, \n",
      "          latest loss 545.084, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 72/150, \n",
      "          latest loss 545.162, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 73/150, \n",
      "          latest loss 545.079, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 74/150, \n",
      "          latest loss 545.077, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 75/150, \n",
      "          latest loss 545.168, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 76/150, \n",
      "          latest loss 545.013, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 77/150, \n",
      "          latest loss 545.097, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 78/150, \n",
      "          latest loss 544.914, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 79/150, \n",
      "          latest loss 544.892, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 80/150, \n",
      "          latest loss 545.019, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 81/150, \n",
      "          latest loss 544.828, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 82/150, \n",
      "          latest loss 544.893, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 83/150, \n",
      "          latest loss 544.897, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 84/150, \n",
      "          latest loss 544.842, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 85/150, \n",
      "          latest loss 544.853, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 86/150, \n",
      "          latest loss 544.882, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 87/150, \n",
      "          latest loss 544.757, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 88/150, \n",
      "          latest loss 544.796, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 89/150, \n",
      "          latest loss 544.793, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 90/150, \n",
      "          latest loss 544.788, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 91/150, \n",
      "          latest loss 544.230, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 92/150, \n",
      "          latest loss 544.694, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 93/150, \n",
      "          latest loss 544.310, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 94/150, \n",
      "          latest loss 544.249, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 95/150, \n",
      "          latest loss 544.362, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 96/150, \n",
      "          latest loss 544.435, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 97/150, \n",
      "          latest loss 544.514, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 98/150, \n",
      "          latest loss 544.445, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 99/150, \n",
      "          latest loss 544.377, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 100/150, \n",
      "          latest loss 544.311, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 101/150, \n",
      "          latest loss 544.266, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 102/150, \n",
      "          latest loss 544.229, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 103/150, \n",
      "          latest loss 544.126, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 104/150, \n",
      "          latest loss 544.096, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 105/150, \n",
      "          latest loss 544.027, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 106/150, \n",
      "          latest loss 543.911, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 107/150, \n",
      "          latest loss 543.863, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 108/150, \n",
      "          latest loss 543.893, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 109/150, \n",
      "          latest loss 543.749, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 110/150, \n",
      "          latest loss 544.162, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 111/150, \n",
      "          latest loss 543.661, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 112/150, \n",
      "          latest loss 543.611, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 113/150, \n",
      "          latest loss 543.496, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 114/150, \n",
      "          latest loss 543.492, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 115/150, \n",
      "          latest loss 543.425, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 116/150, \n",
      "          latest loss 543.468, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 117/150, \n",
      "          latest loss 543.460, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 118/150, \n",
      "          latest loss 543.160, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 119/150, \n",
      "          latest loss 543.073, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 120/150, \n",
      "          latest loss 543.057, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 121/150, \n",
      "          latest loss 542.900, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 122/150, \n",
      "          latest loss 542.810, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 123/150, \n",
      "          latest loss 542.681, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 124/150, \n",
      "          latest loss 542.595, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 125/150, \n",
      "          latest loss 542.636, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 126/150, \n",
      "          latest loss 542.543, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 127/150, \n",
      "          latest loss 542.495, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 128/150, \n",
      "          latest loss 542.461, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 129/150, \n",
      "          latest loss 542.444, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 130/150, \n",
      "          latest loss 542.447, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 131/150, \n",
      "          latest loss 542.590, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 132/150, \n",
      "          latest loss 542.580, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 133/150, \n",
      "          latest loss 542.471, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 134/150, \n",
      "          latest loss 542.450, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 135/150, \n",
      "          latest loss 542.407, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 136/150, \n",
      "          latest loss 542.586, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 137/150, \n",
      "          latest loss 542.512, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 138/150, \n",
      "          latest loss 542.571, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 139/150, \n",
      "          latest loss 542.398, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 140/150, \n",
      "          latest loss 542.455, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.89e+02\n",
      "Finished epoch 141/150, \n",
      "          latest loss 542.855, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 142/150, \n",
      "          latest loss 542.539, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 143/150, \n",
      "          latest loss 542.497, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 144/150, \n",
      "          latest loss 542.480, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 145/150, \n",
      "          latest loss 542.528, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 146/150, \n",
      "          latest loss 542.525, \n",
      "          R^2 on test -0.000, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 147/150, \n",
      "          latest loss 542.518, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 148/150, \n",
      "          latest loss 542.634, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 149/150, \n",
      "          latest loss 542.878, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n",
      "Finished epoch 150/150, \n",
      "          latest loss 542.829, \n",
      "          R^2 on test -0.001, \n",
      "          NMSE on test 3.9e+02\n"
     ]
    }
   ],
   "source": [
    "for n_epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "\n",
    "        y_pred = model(Xbatch)\n",
    "        \n",
    "        ybatch = y[i:i+batch_size]\n",
    "\n",
    "        loss = criterion(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(loss.item())\n",
    "    \n",
    "\n",
    "    print(f'''Finished epoch {n_epoch + 1}/{n_epochs}, \n",
    "          latest loss {loss:.3f}, \n",
    "          R^2 on test {r2_score(y_test.detach().numpy(), model(X_test).detach().floor().numpy()):.3f}, \n",
    "          NMSE on test {mean_squared_error(y_test.detach().numpy(), model(X_test).detach().floor().numpy()):.3}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_conv_14_5_r2_-0.001.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model(X_test).detach().floor().numpy()\n",
    "y_test = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_test, y_test\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=, y=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3b836c7ef044d43059c7846a59355f0b4cbb71a588c1f9492eaa9138c6b55f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
