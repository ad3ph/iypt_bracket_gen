{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cpu')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.genetic_generator import decode_bracket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных\n",
    "\n",
    "TODO: добавить glob, переместить все дампы в папку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810 files\n"
     ]
    }
   ],
   "source": [
    "files_x = sorted(glob.glob('dumps/dump_x*.pkl'))\n",
    "files_y = sorted(glob.glob('dumps/dump_y*.pkl'))\n",
    "print(f'{len(files_x)} files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(files_x[0], 'rb') as fx, open(files_y[0], 'rb') as fy:\n",
    "    data_x = np.array([decode_bracket(i)[0] for i in pickle.load(fx)])\n",
    "    data_y = np.array([decode_bracket(i)[0] for i in pickle.load(fy)])\n",
    "    assert data_x.shape == data_y.shape\n",
    "    assert len(data_x.shape) == 3\n",
    "\n",
    "files_x.pop(0)\n",
    "files_y.pop(0)\n",
    "\n",
    "for filename_x, filename_y in zip(files_x, files_y):\n",
    "    with open(filename_x, 'rb') as fx, open(filename_y, 'rb') as fy:\n",
    "        ex_data_x = np.array([decode_bracket(i)[0] for i in pickle.load(fx)])\n",
    "        ex_data_y = np.array([decode_bracket(i)[0] for i in pickle.load(fy)])\n",
    "\n",
    "        assert ex_data_x.shape == ex_data_y.shape, f'Broke: {ex_data_x}, {ex_data_y}, ex_data_x.shape == ex_data_y.shape: {ex_data_x.shape == ex_data_y.shape}'\n",
    "        assert len(ex_data_x.shape) == 3\n",
    "\n",
    "        data_x = np.vstack((data_x, ex_data_x))\n",
    "        data_y = np.vstack((data_y, ex_data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  8]\n",
      " [56 69]\n",
      " [59 69]\n",
      " ...\n",
      " [34 38]\n",
      " [44 55]\n",
      " [45 55]]\n"
     ]
    }
   ],
   "source": [
    "data_x_r = data_x.reshape((data_x.shape[0], data_x.shape[1] * data_x.shape[2]))\n",
    "data_y_r = data_y.reshape((data_y.shape[0], data_y.shape[1] * data_y.shape[2]))\n",
    "swaps = []\n",
    "\n",
    "for x, y in zip(data_x_r, data_y_r):\n",
    "    swaps.append(np.where(x != y)[0])\n",
    "swaps = np.array(swaps)\n",
    "print(swaps)\n",
    "assert swaps.shape[0] == data_x.shape[0]\n",
    "assert swaps.shape[1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 19510, test: 2168\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(data_x_r.astype('float32'))\n",
    "y = torch.tensor(swaps.astype('float32'))\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(f'Train: {X.shape[0]}, test: {X_test.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полносвязная нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# test activation functions\n",
    "# test architectures\n",
    "# different optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=70, out_features=17, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=17, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Linear(X.shape[1], X.shape[1]),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(X.shape[1], X.shape[1]),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(X.shape[1], X.shape[1]),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(X.shape[1], X.shape[1] // 4),  \n",
    "  nn.ReLU(),\n",
    "  nn.Linear(X.shape[1] // 4, 2),\n",
    "  )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/100, latest loss 1502.338, R^2 on test -2.627, NMSE on test 1.46e+03\n",
      "Finished epoch 2/100, latest loss 1314.386, R^2 on test -2.166, NMSE on test 1.27e+03\n",
      "Finished epoch 3/100, latest loss 1159.766, R^2 on test -1.809, NMSE on test 1.13e+03\n",
      "Finished epoch 4/100, latest loss 1032.555, R^2 on test -1.485, NMSE on test 9.97e+02\n",
      "Finished epoch 5/100, latest loss 927.884, R^2 on test -1.252, NMSE on test 9.04e+02\n",
      "Finished epoch 6/100, latest loss 841.750, R^2 on test -1.040, NMSE on test 8.19e+02\n",
      "Finished epoch 7/100, latest loss 770.861, R^2 on test -0.847, NMSE on test 7.41e+02\n",
      "Finished epoch 8/100, latest loss 712.512, R^2 on test -0.711, NMSE on test 6.87e+02\n",
      "Finished epoch 9/100, latest loss 664.477, R^2 on test -0.553, NMSE on test 6.23e+02\n",
      "Finished epoch 10/100, latest loss 624.927, R^2 on test -0.482, NMSE on test 5.95e+02\n",
      "Finished epoch 11/100, latest loss 592.357, R^2 on test -0.381, NMSE on test 5.54e+02\n",
      "Finished epoch 12/100, latest loss 565.531, R^2 on test -0.322, NMSE on test 5.31e+02\n",
      "Finished epoch 13/100, latest loss 543.430, R^2 on test -0.268, NMSE on test 5.09e+02\n",
      "Finished epoch 14/100, latest loss 525.218, R^2 on test -0.219, NMSE on test 4.89e+02\n",
      "Finished epoch 15/100, latest loss 510.207, R^2 on test -0.175, NMSE on test 4.72e+02\n",
      "Finished epoch 16/100, latest loss 497.831, R^2 on test -0.153, NMSE on test 4.63e+02\n",
      "Finished epoch 17/100, latest loss 487.624, R^2 on test -0.117, NMSE on test 4.48e+02\n",
      "Finished epoch 18/100, latest loss 479.203, R^2 on test -0.102, NMSE on test 4.42e+02\n",
      "Finished epoch 19/100, latest loss 472.253, R^2 on test -0.085, NMSE on test 4.36e+02\n",
      "Finished epoch 20/100, latest loss 466.515, R^2 on test -0.059, NMSE on test 4.25e+02\n",
      "Finished epoch 21/100, latest loss 461.776, R^2 on test -0.059, NMSE on test 4.25e+02\n",
      "Finished epoch 22/100, latest loss 457.859, R^2 on test -0.059, NMSE on test 4.25e+02\n",
      "Finished epoch 23/100, latest loss 454.620, R^2 on test -0.037, NMSE on test 4.16e+02\n",
      "Finished epoch 24/100, latest loss 451.941, R^2 on test -0.037, NMSE on test 4.16e+02\n",
      "Finished epoch 25/100, latest loss 449.723, R^2 on test -0.027, NMSE on test 4.13e+02\n",
      "Finished epoch 26/100, latest loss 447.886, R^2 on test -0.021, NMSE on test 4.1e+02\n",
      "Finished epoch 27/100, latest loss 446.362, R^2 on test -0.021, NMSE on test 4.1e+02\n",
      "Finished epoch 28/100, latest loss 445.099, R^2 on test -0.021, NMSE on test 4.1e+02\n",
      "Finished epoch 29/100, latest loss 444.049, R^2 on test -0.013, NMSE on test 4.07e+02\n",
      "Finished epoch 30/100, latest loss 443.177, R^2 on test -0.013, NMSE on test 4.07e+02\n",
      "Finished epoch 31/100, latest loss 442.452, R^2 on test -0.013, NMSE on test 4.07e+02\n",
      "Finished epoch 32/100, latest loss 441.847, R^2 on test -0.009, NMSE on test 4.05e+02\n",
      "Finished epoch 33/100, latest loss 441.343, R^2 on test -0.009, NMSE on test 4.05e+02\n",
      "Finished epoch 34/100, latest loss 440.922, R^2 on test -0.004, NMSE on test 4.03e+02\n",
      "Finished epoch 35/100, latest loss 440.571, R^2 on test -0.004, NMSE on test 4.03e+02\n",
      "Finished epoch 36/100, latest loss 440.276, R^2 on test -0.004, NMSE on test 4.03e+02\n",
      "Finished epoch 37/100, latest loss 440.029, R^2 on test -0.004, NMSE on test 4.03e+02\n",
      "Finished epoch 38/100, latest loss 439.822, R^2 on test -0.004, NMSE on test 4.03e+02\n",
      "Finished epoch 39/100, latest loss 439.647, R^2 on test -0.004, NMSE on test 4.03e+02\n",
      "Finished epoch 40/100, latest loss 439.500, R^2 on test -0.004, NMSE on test 4.03e+02\n",
      "Finished epoch 41/100, latest loss 439.376, R^2 on test -0.004, NMSE on test 4.03e+02\n",
      "Finished epoch 42/100, latest loss 439.271, R^2 on test -0.004, NMSE on test 4.03e+02\n",
      "Finished epoch 43/100, latest loss 439.182, R^2 on test -0.002, NMSE on test 4.02e+02\n",
      "Finished epoch 44/100, latest loss 439.107, R^2 on test -0.002, NMSE on test 4.02e+02\n",
      "Finished epoch 45/100, latest loss 439.042, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 46/100, latest loss 438.987, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 47/100, latest loss 438.940, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 48/100, latest loss 438.900, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 49/100, latest loss 438.866, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 50/100, latest loss 438.836, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 51/100, latest loss 438.810, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 52/100, latest loss 438.788, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 53/100, latest loss 438.768, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 54/100, latest loss 438.752, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 55/100, latest loss 438.737, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 56/100, latest loss 438.724, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 57/100, latest loss 438.713, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 58/100, latest loss 438.703, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 59/100, latest loss 438.695, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 60/100, latest loss 438.687, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 61/100, latest loss 438.680, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 62/100, latest loss 438.674, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 63/100, latest loss 438.669, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 64/100, latest loss 438.664, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 65/100, latest loss 438.660, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 66/100, latest loss 438.656, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 67/100, latest loss 438.653, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 68/100, latest loss 438.650, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 69/100, latest loss 438.648, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 70/100, latest loss 438.645, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 71/100, latest loss 438.643, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 72/100, latest loss 438.641, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 73/100, latest loss 438.640, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 74/100, latest loss 438.638, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 75/100, latest loss 438.637, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 76/100, latest loss 438.635, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 77/100, latest loss 438.634, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 78/100, latest loss 438.633, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 79/100, latest loss 438.633, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 80/100, latest loss 438.632, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 81/100, latest loss 438.631, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 82/100, latest loss 438.630, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 83/100, latest loss 438.630, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 84/100, latest loss 438.629, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 85/100, latest loss 438.629, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 86/100, latest loss 438.628, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 87/100, latest loss 438.628, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 88/100, latest loss 438.627, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 89/100, latest loss 438.627, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 90/100, latest loss 438.627, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 91/100, latest loss 438.627, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 92/100, latest loss 438.626, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 93/100, latest loss 438.626, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 94/100, latest loss 438.626, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 95/100, latest loss 438.626, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 96/100, latest loss 438.626, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 97/100, latest loss 438.625, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 98/100, latest loss 438.625, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 99/100, latest loss 438.625, R^2 on test -0.000, NMSE on test 4.02e+02\n",
      "Finished epoch 100/100, latest loss 438.625, R^2 on test -0.000, NMSE on test 4.02e+02\n"
     ]
    }
   ],
   "source": [
    "for n_epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        \n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(loss.item())\n",
    "    \n",
    "    print(f'Finished epoch {n_epoch + 1}/{n_epochs}, latest loss {loss:.3f}, R^2 on test {r2_score(y_test.detach().numpy(), model(X_test).detach().floor().numpy()):.3f}, NMSE on test {mean_squared_error(y_test.detach().numpy(), model(X_test).detach().floor().numpy()):.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_big_14.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=70, out_features=70, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=70, out_features=17, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=17, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_big_14.pkl'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(r2_score(y_test.detach().numpy(), model(X_test).detach().floor().numpy()), 3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточная нейронка"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные для нее не reshape'им"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 19510, test: 2168\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor(data_x.astype('float32'))\n",
    "X = X[:, None, :, :]\n",
    "y = torch.tensor(swaps.astype('float32'))\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.1)\n",
    "print(f'Train: {X.shape[0]}, test: {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 8.,  9., 14., 13., 11., 10., 12.,  1.,  5.,  2.,  4.,  6.,  7.,  3.],\n",
       "          [10.,  1.,  2.,  8.,  5., 12., 11.,  3., 13., 14.,  7.,  6.,  4.,  9.],\n",
       "          [ 9., 10.,  5.,  7.,  1.,  6.,  8., 12., 11.,  4.,  2., 14.,  3., 13.],\n",
       "          [ 6.,  5.,  3., 10., 12., 14.,  4.,  2.,  9., 13.,  1., 11.,  8.,  7.],\n",
       "          [13.,  2., 11.,  6.,  8.,  5.,  3.,  1., 12., 10., 14.,  4.,  9.,  7.]]]),\n",
       " tensor([44., 51.]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetSwapAdviser(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNetSwapAdviser, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=4)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=2)\n",
    "        #self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(30, 16)\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "\n",
    "        #x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 30)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 6, 11, 2]             102\n",
      "         MaxPool2d-2              [-1, 6, 5, 1]               0\n",
      "            Linear-3                   [-1, 16]             496\n",
      "            Linear-4                    [-1, 8]             136\n",
      "            Linear-5                    [-1, 2]              18\n",
      "================================================================\n",
      "Total params: 752\n",
      "Trainable params: 752\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ConvNetSwapAdviser().to(device)\n",
    "\n",
    "summary(model, (1, 14, 5))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "history = []\n",
    "n_epochs = 150\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1/150, \n",
      "          latest loss 662.514, \n",
      "          R^2 on test -0.214, \n",
      "          NMSE on test 4.96e+02\n",
      "Finished epoch 2/150, \n",
      "          latest loss 546.718, \n",
      "          R^2 on test -0.009, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 3/150, \n",
      "          latest loss 545.230, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 4/150, \n",
      "          latest loss 545.144, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.11e+02\n",
      "Finished epoch 5/150, \n",
      "          latest loss 545.136, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 6/150, \n",
      "          latest loss 545.135, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 7/150, \n",
      "          latest loss 545.145, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 8/150, \n",
      "          latest loss 545.152, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 9/150, \n",
      "          latest loss 545.164, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 10/150, \n",
      "          latest loss 545.183, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 11/150, \n",
      "          latest loss 545.195, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 12/150, \n",
      "          latest loss 545.204, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 13/150, \n",
      "          latest loss 545.218, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 14/150, \n",
      "          latest loss 545.231, \n",
      "          R^2 on test -0.008, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 15/150, \n",
      "          latest loss 545.244, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 16/150, \n",
      "          latest loss 545.263, \n",
      "          R^2 on test -0.008, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 17/150, \n",
      "          latest loss 545.286, \n",
      "          R^2 on test -0.008, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 18/150, \n",
      "          latest loss 545.317, \n",
      "          R^2 on test -0.008, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 19/150, \n",
      "          latest loss 545.351, \n",
      "          R^2 on test -0.008, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 20/150, \n",
      "          latest loss 545.379, \n",
      "          R^2 on test -0.008, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 21/150, \n",
      "          latest loss 545.406, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 22/150, \n",
      "          latest loss 545.454, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 23/150, \n",
      "          latest loss 545.504, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 24/150, \n",
      "          latest loss 545.552, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 25/150, \n",
      "          latest loss 545.600, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 26/150, \n",
      "          latest loss 545.646, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 27/150, \n",
      "          latest loss 545.686, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 28/150, \n",
      "          latest loss 545.727, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 29/150, \n",
      "          latest loss 545.767, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 30/150, \n",
      "          latest loss 545.799, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 31/150, \n",
      "          latest loss 545.824, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 32/150, \n",
      "          latest loss 545.848, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 33/150, \n",
      "          latest loss 545.874, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 34/150, \n",
      "          latest loss 545.897, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 35/150, \n",
      "          latest loss 545.918, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 36/150, \n",
      "          latest loss 545.947, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 37/150, \n",
      "          latest loss 545.981, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n",
      "Finished epoch 38/150, \n",
      "          latest loss 546.009, \n",
      "          R^2 on test -0.007, \n",
      "          NMSE on test 4.12e+02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 12\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     13\u001b[0m     history\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\u001b[39mFinished epoch \u001b[39m\u001b[39m{\u001b[39;00mn_epoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mn_epochs\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[39m      latest loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39m      R^2 on test \u001b[39m\u001b[39m{\u001b[39;00mr2_score(y_test\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(),\u001b[39m \u001b[39mmodel(X_test)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mfloor()\u001b[39m.\u001b[39mnumpy())\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[39m      NMSE on test \u001b[39m\u001b[39m{\u001b[39;00mmean_squared_error(y_test\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(),\u001b[39m \u001b[39mmodel(X_test)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mfloor()\u001b[39m.\u001b[39mnumpy())\u001b[39m:\u001b[39;00m\u001b[39m.3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'''\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/torch/optim/optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m`requires_grad` is not supported for `step` in differentiable mode\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    232\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 234\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    235\u001b[0m          grads,\n\u001b[1;32m    236\u001b[0m          exp_avgs,\n\u001b[1;32m    237\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    238\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    239\u001b[0m          state_steps,\n\u001b[1;32m    240\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    241\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    242\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    243\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    244\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    245\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    246\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    247\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    248\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    249\u001b[0m          differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    250\u001b[0m          fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    251\u001b[0m          grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    252\u001b[0m          found_inf\u001b[39m=\u001b[39;49mfound_inf)\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/torch/optim/adam.py:300\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 300\u001b[0m func(params,\n\u001b[1;32m    301\u001b[0m      grads,\n\u001b[1;32m    302\u001b[0m      exp_avgs,\n\u001b[1;32m    303\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    304\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    305\u001b[0m      state_steps,\n\u001b[1;32m    306\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    307\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    308\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    309\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    310\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    311\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    312\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    313\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    314\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    315\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    316\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/torch/optim/adam.py:363\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    360\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    362\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    364\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n_epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "\n",
    "        y_pred = model(Xbatch)\n",
    "        \n",
    "        ybatch = y[i:i+batch_size]\n",
    "\n",
    "        loss = criterion(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(loss.item())\n",
    "    \n",
    "\n",
    "    print(f'''Finished epoch {n_epoch + 1}/{n_epochs}, \n",
    "          latest loss {loss:.3f}, \n",
    "          R^2 on test {r2_score(y_test.detach().numpy(), model(X_test).detach().floor().numpy()):.3f}, \n",
    "          NMSE on test {mean_squared_error(y_test.detach().numpy(), model(X_test).detach().floor().numpy()):.3}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_conv_14_5_r2_-0.001.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model(X_test).detach().floor().numpy()\n",
    "y_test = y_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_test, y_test\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=, y=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3b836c7ef044d43059c7846a59355f0b4cbb71a588c1f9492eaa9138c6b55f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
